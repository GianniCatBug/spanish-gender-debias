{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10012a92-ee92-4c0e-becc-7ce44f572d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fed8e32347743dd8562dcaf84d37e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    IntervalStrategy,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftConfig,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "import utils as u\n",
    "\n",
    "MODEL_SEQ2SEQ = \"google/flan-t5-base\"\n",
    "PEFT_MODEL_ID = \"flan-base-4bit-005-gender-debias-spanish\"\n",
    "CORPUS_FILE = \"20231109_gender_bias_dataset.csv\"\n",
    "HF_USER = \"GianniCatBug\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5c437-35fd-4eab-99de-4d3dc577f32c",
   "metadata": {},
   "source": [
    "# Download model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7192387-c920-44df-96ca-cd75cd10ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_SEQ2SEQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5161e342-fd9f-489b-b450-2263a80f0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_SEQ2SEQ,\n",
    "    device_map={\"\":0},\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b7f1b-4723-4c0e-8843-503bdf034ced",
   "metadata": {},
   "source": [
    "## Prepare new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d638b24e-61be-4398-919f-98f68423f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100\n",
      "32128\n",
      "10\n",
      "tensor([[32100,     3, 32101,     3, 32102,     3, 32103,     3, 32104,     3,\n",
      "         32105,     3, 32106,     3, 32107,     3, 32108,     3, 32109,     1]])\n",
      "í ñ ú ¡ Í ¿ Á Ó Ú Ñ</s>\n",
      "32110\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))\n",
    "print(model.get_input_embeddings().num_embeddings)\n",
    "\n",
    "num_added_tokens = tokenizer.add_tokens(u.new_tokens[\"SEQ_2_SEQ\"])\n",
    "print(num_added_tokens)\n",
    "\n",
    "encoding_new_t = tokenizer(\" \".join(u.new_tokens[\"SEQ_2_SEQ\"]), return_tensors=\"pt\")\n",
    "print(encoding_new_t[\"input_ids\"])\n",
    "print(tokenizer.decode(encoding_new_t[\"input_ids\"][0], skip_special_tokens=False))\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model.get_input_embeddings().num_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfd84c-dfcd-4733-99b4-0e5c66340b24",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e22a2d8-6261-423b-8771-687128ebc2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eliminar sesgo de género del siguiente texto:\\...</td>\n",
       "      <td>Chilkatufe UChile mew, estudiantes mapuche U. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eliminar sesgo de género del siguiente texto:\\...</td>\n",
       "      <td>Biblioteca Central, FCFM Académicas mapuche, FCFM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Eliminar sesgo de género del siguiente texto:\\...   \n",
       "1  Eliminar sesgo de género del siguiente texto:\\...   \n",
       "\n",
       "                                              target  \n",
       "0  Chilkatufe UChile mew, estudiantes mapuche U. ...  \n",
       "1  Biblioteca Central, FCFM Académicas mapuche, FCFM  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"../../data/processed/{CORPUS_FILE}\")\n",
    "df[\"input\"] = \"Eliminar sesgo de género del siguiente texto:\\n\" + df[\"input\"]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c68022-093c-4e21-8b0a-d3de17c5b5db",
   "metadata": {},
   "source": [
    "## Get input and output max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19602b7-8a2b-46ed-afc1-875f2c7672d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31195, 2), (21836, 2), (9359, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "df.shape, train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb02af6-3ef1-47d0-a7c3-714831ff692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 228)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_lengths = pd.Series([\n",
    "    len(i)\n",
    "    for i in tokenizer(train_df[\"input\"].to_list())[\"input_ids\"]\n",
    "])\n",
    "max_source_length = int(source_lengths.quantile(0.99))\n",
    "\n",
    "target_lengths = pd.Series([\n",
    "    len(i)\n",
    "    for i in tokenizer(train_df[\"target\"].to_list())[\"input_ids\"]\n",
    "])\n",
    "max_target_length = int(target_lengths.quantile(0.99))\n",
    "\n",
    "max_source_length, max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5464f509-02ad-453c-8053-7d6fde5a502a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    21836.000000\n",
       " mean        80.989101\n",
       " std         50.720423\n",
       " min         20.000000\n",
       " 25%         48.000000\n",
       " 50%         70.000000\n",
       " 75%        101.000000\n",
       " max       2629.000000\n",
       " dtype: float64,\n",
       " count    21836.000000\n",
       " mean        63.413217\n",
       " std         51.077284\n",
       " min          2.000000\n",
       " 25%         30.000000\n",
       " 50%         52.000000\n",
       " 75%         84.000000\n",
       " max       2611.000000\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_lengths.describe(), target_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0185e9-c6e5-4316-955e-e7f1bc5a7b66",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e314dfb6-241a-410e-83d2-25fde51fd99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5257a2678f4352a90df5250602ecef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153b76a3f08a4aad8e4414b6245fec8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9359 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 21836\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 9359\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df).map(\n",
    "    lambda data: u.preprocess_function(\n",
    "        data, tokenizer, max_source_length=max_source_length, max_target_length=max_target_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"target\"]\n",
    ")\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_df).map(\n",
    "    lambda data: u.preprocess_function(\n",
    "        data, tokenizer, max_source_length=max_source_length, max_target_length=max_target_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"target\"]\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2503987-4254-4a4a-b748-a70f38b8ef6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test Raw Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ab7d3c-fa93-4ecf-aecf-0d7cbe1e1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Eliminar sesgo de género del siguiente texto:\n",
    "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿Soy Innovador/a\"?\n",
    "\"\"\"\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26255b1d-80f7-4706-b900-a9b776294d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.01 s, sys: 26.1 ms, total: 2.03 s\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 1000\n",
    "generation_config.num_return_sequences = 1\n",
    "#generation_config.temperature = 0.7\n",
    "generation_config.do_sample = False\n",
    "#generation_config.top_p = 0.9\n",
    "\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67feb958-3587-441a-a7fc-196f135fc0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Eliminar sesgo de género del siguiente texto:\n",
      "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿Soy Innovador/a\"?\n",
      "\n",
      "Raw model generation:\n",
      "Eliminar sexo del siguiente texto: En el Torneo de Innovación Interfacultades UChile te equivocamos invitar a nuestra primer Taller \"soy innovador/a\"?\n",
      "CPU times: user 880 µs, sys: 0 ns, total: 880 µs\n",
      "Wall time: 875 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"Raw model generation:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4e0ee-def0-453f-ace4-5c9e238a0b7e",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3cc1337-6c8c-438b-a7a5-49d554ffd5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 249,319,680 || trainable%: 0.7097201472422875\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    peft_type=\"LORA\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,    # TaskType.SEQ_2_SEQ_LM\n",
    "    r=16,                               # rank, 16 tiene mas accuracy\n",
    "    lora_alpha=32,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q\", \"v\"],          # módulos a los que se les quiere aplicar lora\n",
    "    lora_dropout=0.05                    # prevent overfitting, improve generalization\n",
    ")\n",
    "\n",
    "peft_lora_model = get_peft_model(\n",
    "    prepare_model_for_kbit_training(model),\n",
    "    lora_config\n",
    ")\n",
    "\n",
    "# trainable params: 1,769,472 || all params: 249,347,328 || trainable%: 0.7096414524241463\n",
    "peft_lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbe88687-7a38-4369-8829-5f5dfd9c36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=peft_lora_model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79559ba4-7424-4a91-97a4-0182598ae752",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"../../models/{PEFT_MODEL_ID}..\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_strategy=IntervalStrategy.STEPS,\n",
    "    save_steps=682,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"{HF_USER}/{PEFT_MODEL_ID}\",\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=682,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=peft_lora_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab543af-5d4a-4903-a058-cd15c4b82c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=682,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=GianniCatBug/flan-base-4bit-005-gender-debias-spanish,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/flan-base-4bit-005-gender-debias-spanish../logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=682,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=paged_adamw_8bit,\n",
      "optim_args=None,\n",
      "output_dir=../models/flan-base-4bit-005-gender-debias-spanish..,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/flan-base-4bit-005-gender-debias-spanish..,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=682,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb77346-93a7-40dc-958e-9b7145944606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): T5ForConditionalGeneration(\n",
      "      (shared): Embedding(32110, 768)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32110, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
      "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
      "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32110, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
      "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear4bit(\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
      "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=32110, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(peft_lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac3fa0f5-5870-4921-a4b4-7c6a55da5fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6820' max='6820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6820/6820 3:01:34, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.055490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1364</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.041384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2046</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.036213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2728</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.030398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.030380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4092</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.027713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4774</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.025612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5456</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.025340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6138</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.024781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6820</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.024001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6820, training_loss=0.050108268044211646, metrics={'train_runtime': 10896.2887, 'train_samples_per_second': 10.02, 'train_steps_per_second': 0.626, 'total_flos': 3.647791541256192e+16, 'train_loss': 0.050108268044211646, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_lora_model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd1a39-9992-4d49-aa44-6944718bbd9d",
   "metadata": {},
   "source": [
    "# Save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36d11ff0-3d11-4ef2-b13d-5a3582daae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gianina/.local/lib/python3.10/site-packages/transformers/utils/hub.py:844: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GianniCatBug/flan-base-4bit-005-gender-debias-spanish/commit/ebd1fe6637101850fbf28e6d80f939c08d87a40c', commit_message='Upload model', commit_description='', oid='ebd1fe6637101850fbf28e6d80f939c08d87a40c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(PEFT_MODEL_ID)\n",
    "tokenizer.save_pretrained(PEFT_MODEL_ID)\n",
    "\n",
    "peft_lora_model.push_to_hub(\n",
    "    PEFT_MODEL_ID, use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2a257-43b0-4978-8678-b2f6832c0672",
   "metadata": {},
   "source": [
    "# Test fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4bb293f-7e25-4c73-a9ca-db23fc6706db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan-base-4bit-005-gender-debias-spanish google/flan-t5-base\n",
      "32128\n",
      "32110\n",
      "32110\n",
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained(f\"{HF_USER}/{PEFT_MODEL_ID}\")\n",
    "print(PEFT_MODEL_ID, config.base_model_name_or_path)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map={\"\":0},\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model.get_input_embeddings().num_embeddings)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "_ = tokenizer.add_tokens(new_tokens = u.new_tokens[\"SEQ_2_SEQ\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model.get_input_embeddings().num_embeddings) # 32110\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, f\"{HF_USER}/{PEFT_MODEL_ID}\", device_map={\"\":0})\n",
    "print(model.get_input_embeddings().num_embeddings) # 32110\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "653d59f4-d10a-4d93-841e-ec129e8d4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.max_new_tokens = 1000\n",
    "#generation_config.temperature = 0.7\n",
    "generation_config.do_sample = False\n",
    "#generation_config.top_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5a79a3b-c36b-4a88-b105-6d48f30114a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 103 ms, total: 1.63 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"\"\"Eliminar sesgo de género del siguiente texto:\n",
    "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿Soy Innovador/a\"?\n",
    "\"\"\"\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bee4faf-4d17-491b-b7f9-75fa78452de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Eliminar sesgo de género del siguiente texto:\n",
      "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿Soy Innovador/a\"?\n",
      "\n",
      "Fine tuned model generation:\n",
      "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿ Soy Innovador/a\"? \n",
      "CPU times: user 598 µs, sys: 0 ns, total: 598 µs\n",
      "Wall time: 543 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"Fine tuned model generation:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a756b-9f3b-48d2-8569-f35bc54e7ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
