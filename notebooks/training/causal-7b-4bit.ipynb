{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10012a92-ee92-4c0e-becc-7ce44f572d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4ff559a5b6444abd0e2a5754420a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    IntervalStrategy,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftConfig,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "import utils as u\n",
    "\n",
    "MODEL_CAUSAL = \"vilsonrodrigues/falcon-7b-instruct-sharded\"\n",
    "PEFT_MODEL_ID = \"falcon-7b-4bit-005-gender-debias-spanish\"\n",
    "CORPUS_FILE = \"20231109_gender_bias_dataset.csv\"\n",
    "HF_USER = \"GianniCatBug\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5c437-35fd-4eab-99de-4d3dc577f32c",
   "metadata": {},
   "source": [
    "# Download model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7192387-c920-44df-96ca-cd75cd10ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CAUSAL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5161e342-fd9f-489b-b450-2263a80f0b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████| 15/15 [01:56<00:00,  7.77s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(65024, 4544)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_CAUSAL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2503987-4254-4a4a-b748-a70f38b8ef6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test Raw Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ab7d3c-fa93-4ecf-aecf-0d7cbe1e1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "<human>: ¿Puedes reescribir el siguiente texto sin sesgo de género?\n",
    "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿Soy Innovador/a\"?\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9480ee-054d-4924-b1e1-c338b523dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.num_return_sequences = 1\n",
    "#generation_config.temperature = 0.7\n",
    "generation_config.do_sample = False\n",
    "#generation_config.top_p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26255b1d-80f7-4706-b900-a9b776294d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.93 s, sys: 69 ms, total: 4 s\n",
      "Wall time: 6.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67feb958-3587-441a-a7fc-196f135fc0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<human>: ¿Puedes reescribir el siguiente texto sin sesgo de género?\n",
      "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿Soy Innovador/a\"?\n",
      "<assistant>:\n",
      "\n",
      "Raw model generation:\n",
      "<human>: ¿Puedes reescribir el siguiente texto sin sesgo de género?\n",
      "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿Soy Innovador/a\"?\n",
      "<assistant>: ¿Puedes reescribir el siguiente texto sin sesgo de género?\n",
      "Dentro del Torneo de Innovación Interfacultades UChile tequeremos invitar a nuestro primer Taller \"¿Soy Innovador/a\"?\n",
      "User \n",
      "CPU times: user 349 µs, sys: 209 µs, total: 558 µs\n",
      "Wall time: 435 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nRaw model generation:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfd84c-dfcd-4733-99b4-0e5c66340b24",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87366d3-eef3-4c01-a2f1-33ce49a95aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>input_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chilkatufe UChile mew, estudiantes mapuche U. ...</td>\n",
       "      <td>Chilkatufe UChile mew, estudiantes mapuche U. ...</td>\n",
       "      <td>&lt;human&gt;: ¿Puedes reescribir el siguiente texto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biblioteca Central, FCFM Académicas mapuche, FCFM</td>\n",
       "      <td>Biblioteca Central, FCFM Académicas mapuche, FCFM</td>\n",
       "      <td>&lt;human&gt;: ¿Puedes reescribir el siguiente texto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Chilkatufe UChile mew, estudiantes mapuche U. ...   \n",
       "1  Biblioteca Central, FCFM Académicas mapuche, FCFM   \n",
       "\n",
       "                                              target  \\\n",
       "0  Chilkatufe UChile mew, estudiantes mapuche U. ...   \n",
       "1  Biblioteca Central, FCFM Académicas mapuche, FCFM   \n",
       "\n",
       "                                             input_f  \n",
       "0  <human>: ¿Puedes reescribir el siguiente texto...  \n",
       "1  <human>: ¿Puedes reescribir el siguiente texto...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"../../data/processed/{CORPUS_FILE}\")\n",
    "df[\"input_f\"] = [\n",
    "     f\"\"\"\n",
    "    <human>: ¿Puedes reescribir el siguiente texto sin sesgo de género?\n",
    "    {i}\n",
    "    <assistant>: {t}\n",
    "    <|endoftext|>\"\"\".strip()\n",
    "    for i, t in zip(df[\"input\"], df[\"target\"])\n",
    "]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c69321-e99a-4623-9d3f-e2ed3f5b57e0",
   "metadata": {},
   "source": [
    "## Get input and output max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32824e0f-cf5b-45a3-aa6a-3f26fc1fc859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31195, 3), (21836, 3), (9359, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "df.shape, train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60a2be7-f1a5-493b-874c-b09719e4dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 361.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    21836.000000\n",
       "mean       117.391097\n",
       "std         76.827688\n",
       "min         31.000000\n",
       "25%         71.000000\n",
       "50%        101.000000\n",
       "75%        143.000000\n",
       "max       4925.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_lengths = pd.Series([\n",
    "    len(i)\n",
    "    for i in tokenizer(train_df[\"input_f\"].to_list())[\"input_ids\"]\n",
    "])\n",
    "max_source_length = int(source_lengths.quantile(0.962))\n",
    "print(max_source_length, source_lengths.quantile(0.99))\n",
    "source_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a84ac-1f37-4680-a81f-c05266469c67",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30147f37-4f10-4789-9af1-69e7d60749a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████| 21836/21836 [00:02<00:00, 10123.97 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████| 9359/9359 [00:01<00:00, 9277.55 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 21836\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 9359\n",
       " }))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df).map(\n",
    "    lambda data: tokenizer(\n",
    "        data[\"input_f\"],\n",
    "        max_length=max_source_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device),\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"target\", \"input_f\"]\n",
    ")\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_df).map(\n",
    "    lambda data: tokenizer(\n",
    "        data[\"input_f\"],\n",
    "        max_length=max_source_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device),\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"target\", \"input_f\"]\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4e0ee-def0-453f-ace4-5c9e238a0b7e",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3cc1337-6c8c-438b-a7a5-49d554ffd5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 6,926,439,296 || trainable%: 0.06812435363037071\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        \"query_key_value\",\n",
    "    ],       \n",
    ")\n",
    "\n",
    "peft_lora_model = get_peft_model(\n",
    "    prepare_model_for_kbit_training(model),\n",
    "    lora_config,\n",
    ")\n",
    "\n",
    "peft_lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79559ba4-7424-4a91-97a4-0182598ae752",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"../../models/{PEFT_MODEL_ID}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\tper_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_strategy=IntervalStrategy.STEPS,\n",
    "    save_steps=682,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"{HF_USER}/{PEFT_MODEL_ID}\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=682,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_lora_model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab543af-5d4a-4903-a058-cd15c4b82c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=682,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=GianniCatBug/falcon-7b-4bit-005-gender-debias-spanish,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../../models/falcon-7b-4bit-005-gender-debias-spanish/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=682,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=paged_adamw_8bit,\n",
      "optim_args=None,\n",
      "output_dir=../../models/falcon-7b-4bit-005-gender-debias-spanish,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../../models/falcon-7b-4bit-005-gender-debias-spanish,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=682,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb77346-93a7-40dc-958e-9b7145944606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): FalconForCausalLM(\n",
      "      (transformer): FalconModel(\n",
      "        (word_embeddings): Embedding(65024, 4544)\n",
      "        (h): ModuleList(\n",
      "          (0-31): 32 x FalconDecoderLayer(\n",
      "            (self_attention): FalconAttention(\n",
      "              (maybe_rotary): FalconRotaryEmbedding()\n",
      "              (query_key_value): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4544, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4672, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
      "              )\n",
      "              (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (mlp): FalconMLP(\n",
      "              (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
      "              (act): GELU(approximate='none')\n",
      "              (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
      "            )\n",
      "            (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(peft_lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5ae933-f40e-44ef-b155-2924d8db0b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6820' max='6820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6820/6820 20:26:33, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>1.226200</td>\n",
       "      <td>0.999010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1364</td>\n",
       "      <td>0.964100</td>\n",
       "      <td>0.943550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2046</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.913257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2728</td>\n",
       "      <td>0.880600</td>\n",
       "      <td>0.888140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>0.833800</td>\n",
       "      <td>0.875492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4092</td>\n",
       "      <td>0.820700</td>\n",
       "      <td>0.862699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4774</td>\n",
       "      <td>0.783800</td>\n",
       "      <td>0.859339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5456</td>\n",
       "      <td>0.784900</td>\n",
       "      <td>0.853606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6138</td>\n",
       "      <td>0.763400</td>\n",
       "      <td>0.854031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6820</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>0.854189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gianina/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6820, training_loss=0.8715902557820518, metrics={'train_runtime': 73602.7452, 'train_samples_per_second': 1.483, 'train_steps_per_second': 0.093, 'total_flos': 1.0853572088832e+18, 'train_loss': 0.8715902557820518, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_lora_model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd1a39-9992-4d49-aa44-6944718bbd9d",
   "metadata": {},
   "source": [
    "# Save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36d11ff0-3d11-4ef2-b13d-5a3582daae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gianina/.local/lib/python3.10/site-packages/transformers/utils/hub.py:844: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GianniCatBug/falcon-7b-4bit-005-gender-debias-spanish/commit/10d83fc9c946e87061a52d6876e9542ce692598d', commit_message='Upload model', commit_description='', oid='10d83fc9c946e87061a52d6876e9542ce692598d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(PEFT_MODEL_ID)\n",
    "tokenizer.save_pretrained(PEFT_MODEL_ID)\n",
    "\n",
    "peft_lora_model.push_to_hub(\n",
    "    PEFT_MODEL_ID, use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2a257-43b0-4978-8678-b2f6832c0672",
   "metadata": {},
   "source": [
    "# Test fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4bb293f-7e25-4c73-a9ca-db23fc6706db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falcon-7b-4bit-005-gender-debias-spanish vilsonrodrigues/falcon-7b-instruct-sharded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b76501d92642cab7739c1d3a5287e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "REVISION = \"87ae1730160cf7022b4a02584223fa82f3e6fe52\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(f\"{HF_USER}/{PEFT_MODEL_ID}\", revision=REVISION)\n",
    "print(PEFT_MODEL_ID, config.base_model_name_or_path)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, f\"{HF_USER}/{PEFT_MODEL_ID}\", revision=REVISION, device_map=\"auto\")\n",
    "model.eval()\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0a756b-9f3b-48d2-8569-f35bc54e7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.num_return_sequences = 1\n",
    "#generation_config.temperature = 0.7\n",
    "generation_config.do_sample = False\n",
    "#generation_config.top_p = 0.5\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a79a3b-c36b-4a88-b105-6d48f30114a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 29\n",
      "CPU times: user 2.46 s, sys: 86 ms, total: 2.55 s\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 3.29 s\n",
    "prompt = f\"\"\"\n",
    "<human>: ¿Puedes reescribir el siguiente texto sin sesgo de género?\"\n",
    "Estimados estudiantes: Los alumnos que inician las clases este mes, deben inscribirse en alumnos.uchile.cl\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_parts = prompt.split(\"\\n\")\n",
    "prompt_len = len(\n",
    "    tokenizer(\n",
    "        \" \".join(input_parts[1:-1] if \"<assistant>\" in prompt else input_parts[1:]),\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device).input_ids[0]\n",
    ")\n",
    "generation_config.max_new_tokens = int(prompt_len * 1.5)\n",
    "print(generation_config.max_new_tokens, prompt_len)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19851e1a-8e49-483d-ab0f-afdf9533b7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "<human>: ¿Puedes reescribir el siguiente texto sin sesgo de género?\"\n",
      "Estimados estudiantes: Los alumnos que inician las clases este mes, deben inscribirse en alumnos.uchile.cl\n",
      "<assistant>: Estimad@s estudiantes: Las/os estudiantes que inician las clases este mes, deben inscribirse en alumnos.uchile.cl\n",
      "<assistant>: Estimad@s estudiantes\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs[0]))\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31708dac-089b-4872-ae8d-89570b9c5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_proc = [\n",
    "    o\n",
    "    for o in list(set(\n",
    "        tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"<assistant>: \")[1].split(\"\\n\")\n",
    "    ))\n",
    "    if o\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bee4faf-4d17-491b-b7f9-75fa78452de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<human>: ¿Puedes reescribir el siguiente texto sin sesgo de género?\"\n",
      "Estimados estudiantes: Los alumnos que inician las clases este mes, deben inscribirse en alumnos.uchile.cl\n",
      "<assistant>:\n",
      "\n",
      "Fine tuned model generation:\n",
      "Estimad@s estudiantes: Las/os estudiantes que inician las clases este mes, deben inscribirse en alumnos.uchile.cl\n",
      "CPU times: user 28 µs, sys: 15 µs, total: 43 µs\n",
      "Wall time: 47.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nFine tuned model generation:\")\n",
    "print(max(output_proc, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597e9fb-015c-4d9b-9557-c14fb610bb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
