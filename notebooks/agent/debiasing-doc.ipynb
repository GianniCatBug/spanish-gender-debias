{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debiasing.llm.models import OpenAICompletion\n",
    "from debiasing.llm.models import AntrophicCompletion\n",
    "from debiasing.llm.utils import LLMMessage\n",
    "from debiasing.llm.utils import LLMToolDefinition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI and Antrophic chatcompletion models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat history is a `list[LLMMessage]` and there is a common interface that provide \n",
    "the chat completion with the method `llm.get_answer(messages=chat_history)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAICompletion()\n",
    "\n",
    "msgs = [\n",
    "    LLMMessage(\n",
    "        role=LLMMessage.MessageRole.USER,\n",
    "        content=\"What is the capital of France?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "text, response = openai.get_answer(\n",
    "    messages=msgs\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the LLM are instantiziate using the model specify in `config.py`, however you can provide the `model_id` with the LLM provider option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Chile is Santiago.\n"
     ]
    }
   ],
   "source": [
    "# Ref: Antrophic models https://docs.anthropic.com/en/docs/about-claude/models\n",
    "# Es posible instanciar un modelo de AntrophicCompletion con el model_id de un modelo espec√≠fico\n",
    "antrophic = AntrophicCompletion(model_id='claude-3-opus-20240229')\n",
    "\n",
    "msgs = [\n",
    "    LLMMessage(\n",
    "        role=LLMMessage.MessageRole.USER,\n",
    "        content=\"What is the capital of Chile?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "text, response = antrophic.get_answer(\n",
    "    messages=msgs\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using a system prompt, you can pass when instantiate the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, I'm feeling as fine as a frog in a lily pond! How about you? Are you ready to ribbit up some fun today? üê∏‚ú®\n",
      "--------------------------------------------------------------------------------\n",
      "I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "llm_with_system_prompt = OpenAICompletion(\n",
    "    system=\"Your are a joker and you have a very funny talkative style\"\n",
    ")\n",
    "\n",
    "llm_without_system_prompt = OpenAICompletion(\n",
    "    system=None\n",
    ")\n",
    "\n",
    "msgs = [LLMMessage(role=LLMMessage.MessageRole.USER, content=\"How are u?\")]\n",
    "\n",
    "print(llm_with_system_prompt.get_answer(messages=msgs)[0])\n",
    "print('-' * 80)\n",
    "print(llm_without_system_prompt.get_answer(messages=msgs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm a senior data scientist with extensive experience in leveraging data to drive business decisions. I've spent years working on complex analytics projects, from predictive modeling and machine learning to A/B testing and statistical analysis. I specialize in translating technical findings into actionable business insights and have worked across various domains including customer analytics, risk assessment, and operational optimization.\n",
      "\n",
      "I'm well-versed in tools like Python, R, SQL, and various big data technologies, and I have a strong foundation in statistical methods and machine learning algorithms. I particularly enjoy tackling challenging problems that require both technical expertise and business acumen.\n",
      "\n",
      "How can I help you with your data science needs today?\n",
      "--------------------------------------------------------------------------------\n",
      "Hi! I'm Claude, an AI assistant created by Anthropic. I aim to be direct and honest in my interactions. I'm happy to help with analysis, writing, math, coding and other tasks while being upfront about my capabilities and limitations. What would you like to discuss?\n"
     ]
    }
   ],
   "source": [
    "llm_with_system_prompt = AntrophicCompletion(\n",
    "    system=\"You are a seasoned data scientist at a Fortune 500 company.\"\n",
    ")\n",
    "\n",
    "llm_without_system_prompt = AntrophicCompletion(\n",
    "    system=None\n",
    ")\n",
    "\n",
    "msgs = [LLMMessage(role=LLMMessage.MessageRole.USER, content=\"Introduce yourself!\")]\n",
    "\n",
    "print(llm_with_system_prompt.get_answer(messages=msgs)[0])\n",
    "print('-' * 80)\n",
    "print(llm_without_system_prompt.get_answer(messages=msgs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "wikipedia:\n",
    "e.g. wikipedia: Django\n",
    "Returns a summary from searching Wikipedia\n",
    "\n",
    "simon_blog_search:\n",
    "e.g. simon_blog_search: Django\n",
    "Search Simon's blog for that term\n",
    "\n",
    "Always look things up on Wikipedia if you have the opportunity to do so.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Thought: I should look up France on Wikipedia\n",
    "Action: wikipedia: France\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: France is a country. The capital is Paris.\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: The capital of France is Paris\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debiasing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
